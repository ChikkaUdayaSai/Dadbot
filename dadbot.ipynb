{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3oxVq6vQ76r7"
   },
   "source": [
    "# Dadbot: Dad's memorial bot based on RASA (old style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kIFYA-Kp8aK4"
   },
   "source": [
    "## Starting Jupyter Notebook with necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0_7gOmu0r3v",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import logging, io, json, warnings\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVAF41hr8jU5"
   },
   "source": [
    "# Installations\n",
    "* Rasa\n",
    "* SpaCy Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4049
    },
    "colab_type": "code",
    "id": "UsvAOHF_1dAY",
    "outputId": "f65c2c83-e7ae-46ef-e800-e43dcb854766",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (20.1.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow[tensorflow-addons]==2.1 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow[tensorflow-addons]==2.1\u001b[0m\n",
      "Collecting rasa[spacy]\n",
      "  Using cached rasa-1.10.2-py3-none-any.whl (510 kB)\n",
      "Collecting colorhash<1.1.0,>=1.0.2\n",
      "  Using cached colorhash-1.0.2-py2.py3-none-any.whl (6.0 kB)\n",
      "Collecting networkx<2.5.0,>=2.4.0\n",
      "  Using cached networkx-2.4-py3-none-any.whl (1.6 MB)\n",
      "Collecting pytz<2020.0,>=2019.1\n",
      "  Using cached pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Collecting pymongo[srv,tls]<3.9.0,>=3.8.0\n",
      "  Using cached pymongo-3.8.0.tar.gz (649 kB)\n",
      "Collecting PyJWT<1.8,>=1.7\n",
      "  Using cached PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tqdm<4.46,>=4.31\n",
      "  Using cached tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
      "Requirement already satisfied: absl-py<0.10,>=0.9 in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (from rasa[spacy]) (0.9.0)\n",
      "Collecting apscheduler<3.7,>=3.6\n",
      "  Using cached APScheduler-3.6.3-py2.py3-none-any.whl (58 kB)\n",
      "Collecting coloredlogs<11.0,>=10.0\n",
      "  Using cached coloredlogs-10.0-py2.py3-none-any.whl (47 kB)\n",
      "Collecting sklearn-crfsuite<0.4,>=0.3\n",
      "  Using cached sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Collecting prompt-toolkit<3.0,>=2.0\n",
      "  Using cached prompt_toolkit-2.0.10-py3-none-any.whl (340 kB)\n",
      "Collecting webexteamssdk<1.4.0,>=1.1.1\n",
      "  Using cached webexteamssdk-1.3.tar.gz (56 kB)\n",
      "Collecting tensorflow-estimator==2.1.0\n",
      "  Using cached tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Collecting jsonpickle<1.5,>=1.3\n",
      "  Using cached jsonpickle-1.4.1-py2.py3-none-any.whl (36 kB)\n",
      "Collecting sanic-cors<0.11.0,>=0.10.0b1\n",
      "  Using cached Sanic_Cors-0.10.0.post3-py2.py3-none-any.whl (17 kB)\n",
      "Collecting psycopg2-binary<2.9.0,>=2.8.2\n",
      "  Using cached psycopg2_binary-2.8.5-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "Collecting oauth2client==4.1.3\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting pika<1.2.0,>=1.1.0\n",
      "  Using cached pika-1.1.0-py2.py3-none-any.whl (148 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16 in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (from rasa[spacy]) (1.19.0)\n",
      "Requirement already satisfied: python-dateutil<2.9,>=2.8 in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (from rasa[spacy]) (2.8.1)\n",
      "Collecting colorclass<2.3,>=2.2\n",
      "  Using cached colorclass-2.2.0.tar.gz (17 kB)\n",
      "Requirement already satisfied: packaging<21.0,>=20.0 in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (from rasa[spacy]) (20.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (from rasa[spacy]) (49.2.0.post20200714)\n",
      "Collecting kafka-python<2.0,>=1.4\n",
      "  Using cached kafka_python-1.4.7-py2.py3-none-any.whl (266 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.23 in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (from rasa[spacy]) (2.24.0)\n",
      "Collecting async_generator<1.11,>=1.10\n",
      "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting matplotlib<3.3,>=3.1\n",
      "  Using cached matplotlib-3.2.2-cp38-cp38-manylinux1_x86_64.whl (12.4 MB)\n",
      "Collecting twilio<6.27,>=6.26\n",
      "  Using cached twilio-6.26.3-py2.py3-none-any.whl (979 kB)\n",
      "Requirement already satisfied: attrs<19.4,>=19.3 in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (from rasa[spacy]) (19.3.0)\n",
      "Collecting boto3<2.0,>=1.12\n",
      "  Using cached boto3-1.14.23-py2.py3-none-any.whl (128 kB)\n",
      "Collecting fbmessenger<6.1.0,>=6.0.0\n",
      "  Using cached fbmessenger-6.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting slackclient<3.0.0,>=2.0.0\n",
      "  Using cached slackclient-2.7.2-py2.py3-none-any.whl (70 kB)\n",
      "Collecting terminaltables<3.2.0,>=3.1.0\n",
      "  Using cached terminaltables-3.1.0.tar.gz (12 kB)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.4.1 in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (from rasa[spacy]) (1.4.1)\n",
      "Collecting cloudpickle<1.4,>=1.2\n",
      "  Using cached cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting scikit-learn<0.23,>=0.22\n",
      "  Using cached scikit_learn-0.22.2.post1-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
      "Collecting questionary<1.6.0,>=1.5.1\n",
      "  Using cached questionary-1.5.2-py3-none-any.whl (26 kB)\n",
      "Collecting gevent<1.6,>=1.4\n",
      "  Using cached gevent-1.5.0-cp38-cp38-manylinux2010_x86_64.whl (5.9 MB)\n",
      "Collecting python-socketio<4.6,>=4.4\n",
      "  Using cached python_socketio-4.5.1-py2.py3-none-any.whl (51 kB)\n",
      "Collecting aiohttp<3.7,>=3.6\n",
      "  Using cached aiohttp-3.6.2-py3-none-any.whl (441 kB)\n",
      "Collecting pydot<1.5,>=1.4\n",
      "  Using cached pydot-1.4.1-py2.py3-none-any.whl (19 kB)\n",
      "Collecting mattermostwrapper<2.3,>=2.2\n",
      "  Using cached mattermostwrapper-2.2.tar.gz (2.5 kB)\n",
      "Collecting sanic-jwt<1.5.0,>=1.3.2\n",
      "  Using cached sanic-jwt-1.4.1.tar.gz (19 kB)\n",
      "Requirement already satisfied: jsonschema<3.3,>=3.2 in /home/debian/.conda/envs/rasa-new/lib/python3.8/site-packages (from rasa[spacy]) (3.2.0)\n",
      "Collecting python-telegram-bot<13.0,>=11.1\n",
      "  Using cached python_telegram_bot-12.8-py2.py3-none-any.whl (375 kB)\n",
      "Collecting redis<4.0,>=3.4\n",
      "  Using cached redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
      "Collecting sanic<20.0.0,>=19.12.2\n",
      "  Using cached sanic-19.12.2-py3-none-any.whl (72 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-addons<0.8.0,>=0.7.1 (from rasa[spacy]) (from versions: 0.10.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow-addons<0.8.0,>=0.7.1 (from rasa[spacy])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "python = sys.executable\n",
    "\n",
    "# In your environment run:\n",
    "!{python} -m pip install -U pip;\n",
    "!{python} -m pip install tensorflow[tensorflow-addons]==2.1;\n",
    "!{python} -m pip install rasa[spacy];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "wyCva14-1gD4",
    "outputId": "642d04c1-b9ad-4ed0-fb28-ee9bef21507b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/debian/.conda/envs/rasa-new/bin/python: No module named spacy\r\n"
     ]
    }
   ],
   "source": [
    "!{python} -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7JQlbqR9CHC"
   },
   "source": [
    "## Downloading the Spanish Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "eRmnEdML3OhH",
    "outputId": "cb852307-d652-40c3-cf3d-66c54f833908",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!{python} -m spacy link es_core_news_md es --force;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7JQlbqR9CHC"
   },
   "source": [
    "## Import the Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSw6zFmk3iPu"
   },
   "outputs": [],
   "source": [
    "import rasa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEw5vhSq9gWa"
   },
   "source": [
    "# 1. Teaching the bot to understand user inputs using Rasa NLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieoWk91X9y8X"
   },
   "source": [
    "## Training the NLU Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "dp3AIHmS4L6x",
    "outputId": "8011c4f7-c789-4138-84d7-4710207615d8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rasa.training_data import load_data\n",
    "from rasa.config import RasaNLUModelConfig\n",
    "from rasa.model import Trainer\n",
    "from rasa import config\n",
    "import spacy\n",
    "\n",
    "#spacy_parser = spacy.load('es_core_news_md')\n",
    "#nlp = spacy.load('es')\n",
    "\n",
    "# loading the nlu training samples\n",
    "training_data = load_data(\"data/nlu/nlu-papaito.md\")\n",
    "\n",
    "# trainer to educate our pipeline\n",
    "trainer = Trainer(config.load(\"config_simple.yml\"))\n",
    "\n",
    "# train the model!\n",
    "interpreter = trainer.train(training_data)\n",
    "\n",
    "# store it for future use\n",
    "model_directory = trainer.persist(\"./models/nlu\", fixed_model_name=\"current\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jrfp4xOS95ZZ"
   },
   "source": [
    "## Evaluating the NLU model on a random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "4UjzlqMV4N1k",
    "outputId": "37ea93e5-6a71-4e8e-d2b6-a45144d184ad"
   },
   "outputs": [],
   "source": [
    "# A helper function for prettier output\n",
    "\n",
    "def pprint(o):   \n",
    "    print(json.dumps(o, indent=2))\n",
    "    \n",
    "pprint(interpreter.parse(\"dejándome el coche\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPlSd-As-Fz4"
   },
   "source": [
    "## Evaluating the NLU model on a test data\n",
    "(Here we are using the data at hand i.e nlu.md but it isr recommended to use unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1483
    },
    "colab_type": "code",
    "id": "FmRCylbT4jyw",
    "outputId": "fd1bfd57-ebb3-4541-d3b3-b4cbba781164",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.test import run_evaluation\n",
    "\n",
    "run_evaluation(\"data/nlu/nlu-papaito.md\", model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Av3R2GZZ-WJO"
   },
   "source": [
    "# 2. Teaching the bot to respond using Rasa Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-jn1g_k-o-m"
   },
   "source": [
    "##  Visualising the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXZhTaSw9SNR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#!apt-get -qq install -y graphviz libgraphviz-dev pkg-config;\n",
    "#!breq install graphviz\n",
    "\n",
    "#!conda install -y -n rasa pygraphviz pkg-config;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1090
    },
    "colab_type": "code",
    "id": "O1gYRXe15amU",
    "outputId": "9c0838e3-56c1-4eeb-a879-cc09619269d3"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from rasa.agent import Agent\n",
    " \n",
    "agent = Agent('domain-papaito.yml')\n",
    "#agent.visualize(\"data/core/stories-papaito.md\", \"story_graph.png\", max_history=2)\n",
    "#Image(filename=\"story_graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCdKD3l7-ua8"
   },
   "source": [
    "## Training a Dialogue Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7364
    },
    "colab_type": "code",
    "id": "4D7R-FRO5dxz",
    "outputId": "727adf2a-fa4b-4158-df94-30ad472f62f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rasa.policies import FallbackPolicy, KerasPolicy, MemoizationPolicy, FormPolicy\n",
    "from rasa.agent import Agent\n",
    "\n",
    "# this will catch predictions the model isn't very certain about\n",
    "# there is a threshold for the NLU predictions as well as the action predictions\n",
    "fallback = FallbackPolicy(fallback_action_name=\"utter_unclear\",\n",
    "                          core_threshold=0.2,\n",
    "                          nlu_threshold=0.1)\n",
    "\n",
    "agent = Agent('domain-papaito.yml',\n",
    "              policies=[MemoizationPolicy(max_history=5),\n",
    "                        KerasPolicy(validation_split=0.1,epochs=400),\n",
    "                        FormPolicy(),\n",
    "                        fallback])\n",
    "\n",
    "# loading our neatly defined training dialogues\n",
    "training_data = agent.load_data('data/core/stories-papaito.md')\n",
    "\n",
    "agent.train(training_data)\n",
    "\n",
    "agent.persist('models/dialogue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4911z6y-5rD"
   },
   "source": [
    "# Talk to your Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nE4coPam5hry",
    "outputId": "c8ec135b-882b-4e9e-a955-f3e184177817",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Starting the Bot\n",
    "from rasa.agent import Agent\n",
    "from rasa.utils import EndpointConfig\n",
    "\n",
    "action_endpoint = EndpointConfig(url=\"http://0.0.0.0:5055/webhook\")\n",
    "agent = Agent.load('models/dialogue', interpreter=model_directory, action_endpoint=action_endpoint)\n",
    "\n",
    "#!docker run -d -p 5055:5055 --mount type=bind,source=/home/debian/workspace/Dadbot/actions/actions.py,target=/app/actions rasa/rasa-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the voice synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/NVIDIA/tacotron2.git\n",
    "\n",
    "from tacotron2.hparams import create_hparams\n",
    "from tacotron2.model import Tacotron2\n",
    "from tacotron2.layers import TacotronSTFT, STFT\n",
    "from tacotron2.audio_processing import griffin_lim\n",
    "from tacotron2.train import load_model\n",
    "from tacotron2.text import text_to_sequence\n",
    "from tacotron2.waveglow.mel2samp import files_to_list, MAX_WAV_VALUE\n",
    "from tacotron2.denoiser import Denoiser\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def synthesize(text, voice, sigma=0.6, denoiser_strength=0.05, is_fp16=False):\n",
    "\n",
    "    hparams = create_hparams()\n",
    "    hparams.sampling_rate = 22050\n",
    "\n",
    "    if voice == \"papaito\":\n",
    "        voice_model = \"nvidia_tacotron2_papaito_300\"\n",
    "    elif voice == \"constantino\":\n",
    "        voice_model = \"tacotron2_Constantino_600\"\n",
    "    elif voice == \"orador\":\n",
    "        voice_model = \"checkpoint_tacotron2_29000_es\"\n",
    "    \n",
    "    checkpoint_path = \"/home/debian/workspace/models/\" + voice_model\n",
    "        \n",
    "    model = load_model(hparams)\n",
    "    model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
    "    _ = model.cuda().eval().half()\n",
    "\n",
    "    waveglow_path = '/home/debian/workspace/models/waveglow_256channels_ljs_v2.pt'\n",
    "    waveglow = torch.load(waveglow_path)['model']\n",
    "    _ = waveglow.cuda().eval().half()\n",
    "    denoiser = Denoiser(waveglow)\n",
    "\n",
    "    #text=\"¡Cágate lorito!\"\n",
    "    #with open(filelist_path, encoding='utf-8', mode='r') as f:\n",
    "    #    text = f.read()\n",
    "\n",
    "    sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "    sequence = torch.autograd.Variable(\n",
    "        torch.from_numpy(sequence)).cuda().long()\n",
    "\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
    "    #mel = torch.unsqueeze(mel, 0)\n",
    "    mel = mel_outputs.half() if is_fp16 else mel_outputs\n",
    "    audio = np.array([])\n",
    "    with torch.no_grad():\n",
    "        audio = waveglow.infer(mel, sigma=sigma)\n",
    "        if denoiser_strength > 0:\n",
    "             audio = denoiser(audio, denoiser_strength)\n",
    "        audio = audio * MAX_WAV_VALUE\n",
    "        audio = audio.squeeze()\n",
    "        audio = audio.cpu().numpy()\n",
    "        audio = audio.astype('int16')\n",
    "    \n",
    "    return audio, hparams.sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "DDVLzhAT5yrP",
    "outputId": "aee3fc83-df97-42b4-c7e0-c8929f76337c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from sty import fg, bg, ef, rs\n",
    "\n",
    "print(\"¡Da-bot está listo para cascar! Escribe tus mensajes o dile 'quieto parao'\")\n",
    "while True:\n",
    "    a = input()\n",
    "    \n",
    "    if a == 'quieto parao':\n",
    "        break\n",
    "    responses = agent.handle_text(a)\n",
    "    for response in responses:\n",
    "        to_synth = response[\"text\"]\n",
    "        #to_synth = \"Esto es una prueba para ver si funciona\"\n",
    "        print(fg.blue + to_synth + fg.rs)\n",
    "        response_file = open('response.txt','w') \n",
    "        response_file.write(to_synth)\n",
    "        voice, sr = synthesize(to_synth, \"orador\")\n",
    "        sd.play(voice, sr)\n",
    "        response_file.close()        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Conversational_Chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
